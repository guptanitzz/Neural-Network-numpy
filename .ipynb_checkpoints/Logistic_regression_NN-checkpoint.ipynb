{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook implements basic building blocks of Neural Network with simple Logistic Regression example\n",
    "\n",
    "* Block includes \n",
    "- initialize with zeros - initialize parameter W and b with zeros\n",
    "\n",
    "iterate in a loop for gradient descent computation\n",
    "- forward propagation - get Z and A; where Z= np.dot(W.T,X)+b and A is sigmoid(Z)\n",
    "- compute cost - log loss\n",
    "- backward propagation - get dZ, dW and db \n",
    "- update parameter - update W as W- alpha * dW and db as db- alpha * db where alpha is learning rate of gradient descent algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def L1_loss(y,yhat):\n",
    "    return np.sum(np.abs(y-yhat))\n",
    "\n",
    "def L2_loss(y,yhat):\n",
    "    return np.sum(np.square(y-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "def initialize_with_zeros(dim):\n",
    "    W=np.zeros((dim,1))\n",
    "    b=0  \n",
    "#     print(W.shape,b.shape)\n",
    "    assert(W.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return W,b\n",
    "\n",
    "def forward_propagation(W,X,b):\n",
    "    Z=np.dot(W.T,X)+b\n",
    "    A=sigmoid(Z)\n",
    "    return Z,A\n",
    "\n",
    "def backward_propagation(A,Y,X):\n",
    "    m=X.shape[1]     \n",
    "    dZ=A-Y\n",
    "    cost=compute_cost(Y,A,m)\n",
    "    dW=1/m*np.dot(X,dZ.T)\n",
    "    db=1/m*np.sum(dZ)\n",
    "#     print(db)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ()) \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    return grads,cost\n",
    "    \n",
    "def compute_cost(Y,A,m):\n",
    "    cost=(- 1 / m) * np.sum(Y * np.log(A+epsilon) + (1 - Y) * (np.log(1 - A+epsilon))) \n",
    "    return cost\n",
    "    \n",
    "def update_params(W,b,grads,learning_rate=0.1):\n",
    "    dW = grads[\"dW\"]\n",
    "    db = grads[\"db\"]\n",
    "    W = W - learning_rate * dW  # need to broadcast\n",
    "    b = b - learning_rate * db\n",
    "    return W,b\n",
    "\n",
    "def optimize(W,b,X,Y,num_iterations,learning_rate):\n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        Z,A=forward_propagation(W,X,b)\n",
    "        grads,cost=backward_propagation(A,Y,X)\n",
    "        W,b=update_params(W,b,grads,learning_rate)\n",
    "        costs.append(cost)\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    #final params dict\n",
    "    params = {\"W\": W,\n",
    "              \"b\": b}\n",
    "    return params,grads,costs\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=5, learning_rate=0.5):\n",
    "    dim=X_train.shape[0]\n",
    "    W,b=initialize_with_zeros(dim)\n",
    "    params,grads,costs=optimize(W,b,X_train,Y_train,num_iterations,learning_rate)\n",
    "    W = params[\"W\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    Y_prediction_test = predict(W, b, X_test)\n",
    "    Y_prediction_train = predict(W, b, X_train)\n",
    "    \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"W\" : W, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='Wine-source'\n",
    "datapath = os.path.join('D:','\\Learning','General','data',project)\n",
    "data = pd.read_csv(os.path.join(datapath,'wine.data'),header=None)\n",
    "data.columns = [  'name'\n",
    "                 ,'alcohol',\n",
    "                'malicAcid',\n",
    "                'ash'\n",
    "                ,'ashalcalinity'\n",
    "                ,'magnesium'\n",
    "                ,'totalPhenols'\n",
    "                ,'flavanoids'\n",
    "                ,'nonFlavanoidPhenols'\n",
    "                ,'proanthocyanins'\n",
    "                ,'colorIntensity'\n",
    "                ,'hue'\n",
    "                ,'od280_od315'\n",
    "                ,'proline'\n",
    "                ]\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name']=[1 if x==1 else 0 for x in data['name'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 13)\n",
      "(142,)\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, data.columns != 'name']\n",
    "y = data['name']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 75.35211267605634 %\n",
      "test accuracy: 77.77777777777777 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitika Gupta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_set_x=X_train.T.values\n",
    "train_set_y=y_train.values.reshape(y_train.shape[0],1).T\n",
    "test_set_x=X_test.T.values\n",
    "test_set_y=y_test.values.reshape(y_test.shape[0],1).T\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 200, learning_rate = 0.1)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
